{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://github.com/Harvard-IACS/2021-s109a/blob/master/lectures/crest.png?raw=true\"> CS-S109A Introduction to Data Science \n",
    "\n",
    "## Lecture 10 (Interpreting Machine Learning Models and Randomization Tests)\n",
    "\n",
    "**Harvard University**<br>\n",
    "**Summer 2021**<br>\n",
    "**Instructor:** Kevin Rader<br>\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents \n",
    "<ol start=\"0\">\n",
    "<li> Learning Goals </li>\n",
    "<li> Interpreting Models </li> \n",
    "<li> LIME </li> \n",
    "<li> Randomization Testing </li> \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Goals\n",
    "\n",
    "This Jupyter notebook accompanies Lecture 10. By the end of this notebook, you should be able to:\n",
    "\n",
    "- Interpret the results of machine learning models using several methods. \n",
    "- Use LIME and ELI5 packages.\n",
    "- Investigate the use of randomization testing for AB testing data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "#from sklearn.decomposition import PCA\n",
    "from sklearn import tree\n",
    "from sklearn import ensemble\n",
    "\n",
    "# Here are the decision trees\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "\n",
    "# sns.set(style=\"ticks\")\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today, we will be using the `Heart.csv` data set we've seen many times before.  We are trying to perform analyses to predict `AHD` frmo the other predictors.  We start by reading in the data, loo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_df = pd.read_csv('../data/Heart.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(heart_df.shape)\n",
    "heart_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = heart_df[['Age','Sex','ChestPain','RestBP','Chol','Fbs','RestECG','MaxHR','ExAng','Oldpeak','Slope','Ca','Thal']]\n",
    "y = 1*(heart_df['AHD']=='Yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X['ChestPain']=X['ChestPain'].astype('category')\n",
    "#X['ChestPain']=X['ChestPain'].cat.codes\n",
    "\n",
    "#X['Thal']=X['Thal'].astype('category')\n",
    "#X['Thal']=X['Thal'].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.assign(ChestPain=X['ChestPain'].astype('category').cat.codes)\n",
    "X = X.assign(Thal=X['Thal'].astype('category').cat.codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.describe()\n",
    "X['Ca']=X['Ca'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "itrain, itest = train_test_split(range(X.shape[0]), train_size=0.80)\n",
    "\n",
    "X_train = X.iloc[itrain, :]\n",
    "X_test = X.iloc[itest, :]\n",
    "y_train = y.iloc[itrain]\n",
    "y_test = y.iloc[itest]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.1**: How were the categorical variables handled?  How were missing values treated?  Were these wise choices?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Fitting Four Untuned ML Models\n",
    "\n",
    "Start with 2 decision tree models and evaluate using AUC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a possibly underfit (depth = 3) decision tree classifier\n",
    "dt3 = tree.DecisionTreeClassifier(max_depth = 3)\n",
    "dt3.fit(X_train,y_train)\n",
    "\n",
    "# fit an overfit (depth = 10) decision tree classifier\n",
    "dt10 = tree.DecisionTreeClassifier(max_depth = 10)\n",
    "dt10.fit(X_train,y_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate using AUC\n",
    "\n",
    "print(\"AUC on train for dt3:\",sk.metrics.roc_auc_score(y_train,dt3.predict_proba(X_train)[:,1]))\n",
    "print(\"AUC on test for dt3:\",sk.metrics.roc_auc_score(y_test,dt3.predict_proba(X_test)[:,1]))\n",
    "\n",
    "print(\"AUC on train for dt10:\",sk.metrics.roc_auc_score(y_train,dt10.predict_proba(X_train)[:,1]))\n",
    "print(\"AUC on test for dt10:\",sk.metrics.roc_auc_score(y_test,dt10.predict_proba(X_test)[:,1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now fit the two ensemble models: Rnadom Forest and Boosing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit random forest and adaboost models\n",
    "\n",
    "np.random.seed(109)\n",
    "randomforest = RandomForestClassifier(n_estimators=100, max_features='sqrt', max_depth=10)\n",
    "randomforest.fit(X_train,y_train);\n",
    "\n",
    "adaboost = AdaBoostClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(max_depth=3),\n",
    "    n_estimators=1000,\n",
    "    learning_rate=.8)\n",
    "adaboost.fit(X_train,y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate using AUC\n",
    "print(\"AUC on train for randomforest:\",sk.metrics.roc_auc_score(---,---)\n",
    "print(\"AUC on test for randomforest:\",sk.metrics.roc_auc_score(---,---)\n",
    "\n",
    "print(\"AUC on train for adaboost:\",sk.metrics.roc_auc_score(---,---)\n",
    "print(\"AUC on test for adaboost:\",sk.metrics.roc_auc_score(---,---)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.1**: Which model performs best?  Which models are overfit?  How do you know?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Variable Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the blanks below to calculate the variable importances from the 4 untuned models above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Default Variable Importance\n",
    "\n",
    "plt.figure(figsize=(24,6))\n",
    "#plt.set_xticks()\n",
    "#plt.set_xticklabels(X.columns)\n",
    "num=10 \n",
    "\n",
    "plt.subplot(1, 4, 1)\n",
    "dt3_importances = dt3.feature_importances_\n",
    "order = np.flip(np.argsort(dt3_importances))[0:num]\n",
    "plt.barh(range(num),dt3_importances[order],tick_label=X.columns[order]);\n",
    "plt.title(\"Relative Variable Importance for dt3\")\n",
    "\n",
    "plt.subplot(1, 4, 2)\n",
    "dt10_importances = dt10.feature_importances_\n",
    "order = np.flip(np.argsort(dt10_importances))[0:num]\n",
    "plt.barh(range(num),dt10_importances[order],tick_label=X.columns[order]);\n",
    "plt.title(\"Relative Variable Importance for dt10\")\n",
    "\n",
    "plt.subplot(1, 4, 3)\n",
    "rf_importances = ---\n",
    "order = ---\n",
    "plt.barh(---,---);\n",
    "plt.title(\"Relative Variable Importance for rf\")\n",
    "\n",
    "plt.subplot(1, 4, 4)\n",
    "adaboost_importances = adaboost.feature_importances_\n",
    "adaboost_importances = pd.Series(adaboost_importances).fillna(0)\n",
    "order = np.flip(np.argsort(adaboost_importances))[0:num]\n",
    "plt.barh(range(num),adaboost_importances[order],tick_label=X.columns[order]);\n",
    "plt.title(\"Relative Variable Importance for adaboost\");\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3.1**: How do these variable importance measures compare for these 4 models?  Which predictor is most important in general?  How is it related to `AHD`? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Using Eli-5 \n",
    "\n",
    "We will Explain It Like a 5 year old using `ELI-5` to calculate permutation importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install eli5\n",
    "!pip install eli5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import eli5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#permutation importance for the random forest\n",
    "from eli5.sklearn import PermutationImportance\n",
    "\n",
    "seed = 42\n",
    "\n",
    "perm = PermutationImportance(randomforest,random_state=seed,n_iter=10).fit(X_test, y_test)\n",
    "eli5.show_weights(perm,feature_names=X.columns.tolist())\n",
    "#eli5.explain_weights(perm, feature_names = X_train.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4.1**: Calculate and print out the permutation importances for the adaboost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########\n",
    "# your code below\n",
    "########"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4.2**: How do the permutation importance measures compare to the default variable importance in the random forest?  How does the NN model compare to the random forest?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Interpretation through Prediction Plots\n",
    "\n",
    "We start by plotting hte predictions for all the observed data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "yhat_rf_train = randomforest.predict_proba(X_train)[:,1]\n",
    "plt.scatter(X_train[['Age']],yhat_rf_train);\n",
    "yhat_rf_test = randomforest.predict_proba(X_test)[:,1]\n",
    "plt.scatter(X_test[['Age']],yhat_rf_test,marker='x');\n",
    "plt.title(\"Predicted Probabilities vs. Age from the RF in train and test\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Edit the code below for the adaboost model\n",
    "\n",
    "yhat_adaboost_train = adaboost.predict_proba(---)\n",
    "plt.scatter(---,---);\n",
    "yhat_adaboost_test = adaboost.predict_proba(---)\n",
    "plt.scatter(---,---);\n",
    "plt.title(\"Predicted Probabilities vs. Age from The adaboost model in train and test\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q5.1** How do the random forest and boosted models compare in the interpretation of Age with AHD?  Which is more reliable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the data frame of means to do the prediction\n",
    "means1 = X_train.mean(axis = 0)\n",
    "means_df = (means1.to_frame()).transpose()\n",
    "\n",
    "# Do the prediction at all observed ages\n",
    "Ages = np.arange(np.min(X['Age']),np.max(X['Age']))\n",
    "means_df  = pd.concat([means_df]*Ages.size,ignore_index=True)\n",
    "means_df['Age'] = Ages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plots at means\n",
    "yhat_rf = randomforest.predict_proba(means_df)[:,1]\n",
    "plt.scatter(X_train['Age'],y_train)\n",
    "plt.plot(means_df['Age'],yhat_rf,color=\"red\")\n",
    "plt.title(\"Predicted Probabilities vs. Age from NN in train\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plots for all observations.  And then averaged\n",
    "\n",
    "yhat_rfs = []\n",
    "for i in range(0,X_train.shape[0]):\n",
    "    obs = X_train.iloc[i,:].to_frame().transpose()\n",
    "    obs_df  = pd.concat([obs]*Ages.size,ignore_index=True)\n",
    "    obs_df['Age'] = Ages\n",
    "    yhat_rf = randomforest.predict_proba(obs_df)[:,1]\n",
    "    yhat_rfs.append(yhat_rf)\n",
    "    plt.plot(obs_df['Age'],yhat_rf,color='blue',alpha=0.05)\n",
    "\n",
    "plt.plot(obs_df['Age'],np.mean(yhat_rfs, axis=0),color='red',linewidth=2);\n",
    "    \n",
    "plt.ylim(0,1)\n",
    "plt.title(\"Predicted Probabilities vs. Age from RF in train for all observations\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the 90% prediction interval\n",
    "plt.plot(obs_df['Age'],np.median(yhat_rfs,axis=0),color='red');\n",
    "plt.plot(obs_df['Age'],np.quantile(yhat_rfs,q=.05,axis=0),color='blue');\n",
    "plt.plot(obs_df['Age'],np.quantile(yhat_rfs,q=.95,axis=0),color='blue');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q5.2** Interpret the two plots above.  What is the difference in the interpretations?  Is there any evidence of interaction effects between Age and the other predictors?  How do you know?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Using LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lime\n",
    "import lime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "#explainer = LimeTabularExplainer(X_train)#class_names = [0,1])\n",
    "\n",
    "explainer = LimeTabularExplainer(X_train.values,\n",
    "                                 feature_names=X_train.columns,\n",
    "                                 class_names = [0,1],\n",
    "                                 mode='classification')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 42\n",
    "\n",
    "exp = explainer.explain_instance(X_train.values[idx], \n",
    "                                 randomforest.predict_proba, \n",
    "                                 num_features = 13)#X_train.values[idx].size)\n",
    "\n",
    "print('Observation #: %d' % idx)\n",
    "print('Probability(AHD) =', randomforest.predict_proba(X_train)[idx][1])\n",
    "print('True class: %s' % y_train[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot the results\n",
    "# exp.as_list()\n",
    "exp.as_pyplot_figure();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the observation number and see what changes.\n",
    "idx = ---\n",
    "exp = explainer.explain_instance(X_train.values[idx], \n",
    "                                 randomforest.predict_proba, \n",
    "                                 num_features = 13)\n",
    "\n",
    "print('Observation #: %d' % idx)\n",
    "print('Probability(AHD) =', randomforest.predict_proba(X_train)[idx][1])\n",
    "print('True class: %s' % y_train[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot the results\n",
    "# exp.as_list()\n",
    "exp.as_pyplot_figure();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q6.1** Interpret the LIME results above.  Do they agree with the other interpretations for the random forest model seen so far?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Randomization Testing\n",
    "\n",
    "This part will investigate the power of performing a randomization test for comparing a response `y` between two groups (defined by `x`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we create the mythical data\n",
    "\n",
    "diff = 0\n",
    "n = 100\n",
    "sd = 10\n",
    "\n",
    "x = np.random.binomial(1,0.5,n)\n",
    "y = np.random.normal(diff*x,sd,n)\n",
    "\n",
    "df = pd.DataFrame(np.array([x,y]).T, columns = [\"x\",\"y\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q7.1** Perform a permutation test (called a randomization test in this problem) on the data above.  What do you conclude?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replicates = 100\n",
    "\n",
    "### your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q7.2** Change the value of `diff` to reasonable values (start with 1). What do you conclude?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replicates = 100\n",
    "\n",
    "### your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q7.3** Replicate this data creation and permutation test 200 times (200 separate `experiments`) with `diff = 0`.  How often do you reject the null?  What happens as `diff` increases?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replicates = 100\n",
    "experiments = 200\n",
    "\n",
    "### your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
