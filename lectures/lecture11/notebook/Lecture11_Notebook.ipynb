{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://github.com/Harvard-IACS/2021-s109a/blob/master/lectures/crest.png?raw=true\"> CS-S109A Introduction to Data Science \n",
    "\n",
    "## Lecture 11: Clustering, Missingness, and Wrapup\n",
    "\n",
    "**Harvard University**<br>\n",
    "**Summer 2021**<br>\n",
    "**Instructors:** Kevin Rader<br>\n",
    "**Authors:** Rahul Dave, David Sondak, Pavlos Protopapas, Chris Tanner, Eleni Kaxiras, Kevin Rader\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RUN THIS CELL TO GET THE RIGHT FORMATTING \n",
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get(\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\").text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents \n",
    "<ol start=\"0\">\n",
    "<li> Learning Goals </li> \n",
    "<li> Clustering </li> \n",
    "<li> Missingness and Imputation  </li> \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer, MissingIndicator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Goals\n",
    "\n",
    "This Jupyter notebook accompanies Lecture 11. By the end of this lecture, you should be more comfortable with:\n",
    "\n",
    "- performing basic unsupervised clustering algorithms (k-means and hierarchical)\n",
    "- using basic imputation models to handle missingness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Data \n",
    "\n",
    "For this section of the notebook we will be using 2 **unrelated** data sets:\n",
    "\n",
    "1. The classic Fisher's Iris data set: [Wiki reference](https://en.wikipedia.org/wiki/Iris_flower_data_set)\n",
    "2. `receiving_2020.csv`: NFL receiving statistics: [Source](https://www.pro-football-reference.com/years/2020/receiving.htm).  Note: receivers with fewer than 10 yards were removed.\n",
    "\n",
    "Let's take a peak at them both:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, the common iris data set (from sklearn)\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "X = pd.DataFrame(iris.data)  \n",
    "X.columns = iris.feature_names\n",
    "y = iris.target\n",
    "print(iris.target_names)\n",
    "np.unique(y,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape, y.shape)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wr = pd.read_csv('../data/receiving_2020.csv')\n",
    "print(wr.shape)\n",
    "wr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1a: *K*-Means Clustering\n",
    "\n",
    "We first attempt *K*-Means clustering on the iris data set, which has a clear response variable (iris type).  Let's see if we can recover the three types through unsupervised clustering.  This is done using [`KMeans`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) in `sklearn.cluster`.\n",
    "\n",
    "Let's start with $K=2$...unstandardized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans2 = KMeans(n_clusters=2, random_state=109).fit(X[['sepal length (cm)','sepal width (cm)']])\n",
    "\n",
    "# sum of squares from the centroids\n",
    "print(\"Sum of Squared Distances =\", kmeans2.inertia_)\n",
    "\n",
    "#predict for new (or current) observations\n",
    "print(\"Predicted clusters for each observation =\", kmeans2.predict(X[['sepal length (cm)','sepal width (cm)']]))\n",
    "# print(kmeans2.labels_): predict will match this for the training set\n",
    "\n",
    "#the centroids\n",
    "print(\"Centroid vectors =\", kmeans2.cluster_centers_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X['sepal length (cm)'],X['sepal width (cm)'],c=kmeans2.labels_);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.1** Edit the code above to re-run the analysis with various different seeds.  Do the results change?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.2** Edit the code above to re-run the analysis using the standardized predictors (the first two) and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########\n",
    "# your code here\n",
    "########\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.3** Re-run the analysis from above for $K=3$ and $K=4$.  Which appears to perform the best based on the scatterplots and based on the elbow method?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########\n",
    "# edit and add to the code below\n",
    "########\n",
    "\n",
    "kmeans2 = KMeans(n_clusters=2, random_state=109).fit(X[['sepal length (cm)','sepal width (cm)']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the three scatterplots like the one from above\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize = (18,5))\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot (y = Sum of Squared Distances) vs. (x = K) and evaluate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.4** Compare the $K=3$ model with the actual 3 classes (`pd.crosstab` may be helpful).  How did we perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######\n",
    "# Your code here\n",
    "#######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.5** Perform a *K*-Means clustering analysis on the receiving data using the following variables: `['age','before_catch_perc','depth_of_target','broken_tackles_per_reception','drop_perc','catch_perc','td_per_reception','firstdowns_per_reception','fum_per_reception']`.  Consider using `Ks = np.arange(2,15,1)`.  Which $K$ would you select based on the elbow method?\n",
    "\n",
    "Note: these variables were specifically chosen because they are not directly related to receiving ability...more a representation of play style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######\n",
    "# Your code here\n",
    "#######\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.6** Investigate the top 3 players in each of the clusters.  Are there any general patterns you notice (especially in the variables like `yards`, `rank`, and `pos`)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######\n",
    "# Your code here\n",
    "#######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Side note: here's how you fit a Hierarchical Clustering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fitting a hierarchical clustering\n",
    "hier_cluster = AgglomerativeClustering(distance_threshold=0, n_clusters=None).fit(X)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Dealing with Missingness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Here, we create data in which the true theoretical regression line is:\n",
    "$$ Y = 3X_1 + 2X_2 + \\varepsilon,\\hspace{0.1in} \\varepsilon \\sim N(0,1)$$\n",
    "\n",
    "Note: $\\rho_{X1,X2} = 0.5$\n",
    "\n",
    "We will be inserting missingness into `x1` in various ways, and analyzing the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 500\n",
    "np.random.seed(109)\n",
    "\n",
    "x1 = np.random.normal(0,1,size=n)\n",
    "x2 = 0.5*x1+np.random.normal(0,np.sqrt(0.75),size=n)\n",
    "X = pd.DataFrame(data=np.transpose([x1,x2]),columns=[\"x1\",\"x2\"])\n",
    "\n",
    "y = 3*x1 - 2*x2 + np.random.normal(0,1,size=n)\n",
    "y = pd.Series(y)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(data=np.transpose([x1,x2,y]),columns=[\"x1\",\"x2\",\"y\"])\n",
    "\n",
    "# Checking the correlation\n",
    "scipy.stats.pearsonr(x1,x2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1,ax2,ax3) =  plt.subplots(1, 3, figsize = (18,5))\n",
    "ax1.scatter(x1,y)\n",
    "ax2.scatter(x2,y)\n",
    "ax3.scatter(x2,x1,color=\"orange\")\n",
    "ax1.set_title(\"y vs. x1\")\n",
    "ax2.set_title(\"y vs. x2\")\n",
    "ax3.set_title(\"x1 vs. x2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poke holes in $X_1$ in 3 different ways (all roughly 20% of data are removed): \n",
    "\n",
    "- MCAR: just take out a random sample of 20% of observations in $X_1$\n",
    "- MAR: missingness in  $X_1$ depends on $X_2$, and thus can be recovered in some way\n",
    "- MNAR: missingness in  $X_1$ depends on $X_1$, and thus can be recovered in some way\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1_mcar = x1.copy()\n",
    "x1_mar = x1.copy()\n",
    "x1_mnar = x1.copy()\n",
    "\n",
    "#missing completely at random\n",
    "miss_mcar = np.random.choice(n,int(0.2*n),replace=False)\n",
    "x1_mcar[miss_mcar] = np.nan\n",
    "\n",
    "#missing at random: one way to do it\n",
    "miss_mar = np.random.binomial(1,0.05+0.85*(x2>(x2.mean()+x2.std())),n)\n",
    "x1_mar[miss_mar==1] = np.nan\n",
    "\n",
    "#missing not at random: one way to do it\n",
    "miss_mnar = np.random.binomial(1,0.05+0.85*(y>(y.mean()+y.std())),n)\n",
    "x1_mnar[miss_mnar==1] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the 3 datasets with missingness\n",
    "df_mcar = df.copy()\n",
    "df_mar = df.copy()\n",
    "df_mnar = df.copy()\n",
    "\n",
    "# plug in the appropriate x1 with missingness\n",
    "df_mcar['x1'] = x1_mcar\n",
    "df_mar['x1'] = x1_mar\n",
    "df_mnar['x1'] = x1_mnar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no missingness: on the full dataset\n",
    "ols = LinearRegression().fit(df[['x1','x2']],df['y'])\n",
    "print(ols.intercept_,ols.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the linear regression blindly on the dataset with MCAR missingness, see what happens\n",
    "LinearRegression().fit(df_mcar[['x1','x2']],df_mcar['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1** Why aren't the estimates exactly $\\hat{\\beta}_1 = 3$ and $\\hat{\\beta}_2 = -2$ ?  How does sklearn handle missingness?  What would be a first naive approach to handling missingness?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What happens when you just drop rows?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no missingness for comparison sake\n",
    "ols = LinearRegression().fit(X,y)\n",
    "print(ols.intercept_,ols.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# MCAR: drop the rows that have any missingness\n",
    "ols_mcar = LinearRegression().fit(df_mcar.dropna()[['x1','x2']],df_mcar.dropna()['y'])\n",
    "print(ols_mcar.intercept_,ols_mcar.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAR: drop the rows that have any missingness\n",
    "ols_mar = LinearRegression().fit(df_mar.dropna()[['x1','x2']],df_mar.dropna()['y'])\n",
    "print(ols_mcar.intercept_,ols_mar.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNAR: drop the rows that have any missingness\n",
    "X_mnar_raw = X.copy()\n",
    "X_mnar_raw['x1'] = x1_mnar\n",
    "X_mnar = X.iloc[miss_mnar==0]\n",
    "y_mnar = y[miss_mnar==0]\n",
    "\n",
    "ols_mnar = LinearRegression().fit(X_mnar,y_mnar)\n",
    "print(ols_mnar.intercept_,ols_mnar.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2** How do the estimates compare when just dropping rows?  Are they able to recover the values of $\\beta_1$ that they should?  In which form of missingness is the result the worst?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Start Imputing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make back-=up copies for later since we'll have lots of imputation approaches.\n",
    "X_mcar_raw = X_mcar.copy()\n",
    "X_mar_raw = X_mar.copy()\n",
    "X_mnar_raw = X_mnar.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Imputation:\n",
    "\n",
    "Perform mean imputation using the `fillna`, `dropna`, and `mean` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mcar = X_mcar_raw.copy()\n",
    "X_mcar['x1'] = X_mcar['x1'].fillna(X_mcar['x1'].dropna().mean())\n",
    "\n",
    "ols_mcar_mean = LinearRegression().fit(X_mcar,y)\n",
    "print(ols_mcar_mean.intercept_,ols_mcar_mean.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mar = X_mar_raw.copy()\n",
    "\n",
    "\n",
    "X_mar['x1'] = X_mar['x1'].fillna(X_mar['x1'].dropna().mean())\n",
    "\n",
    "ols_mar_mean = LinearRegression().fit(X_mar,y)\n",
    "print(ols_mar_mean.intercept_,ols_mar_mean.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mnar = X_mnar_raw.copy()\n",
    "X_mnar['x1'] = X_mnar['x1'].fillna(X_mnar['x1'].dropna().mean())\n",
    "\n",
    "ols_mnar_mean = LinearRegression().fit(X_mnar,y)\n",
    "print(ols_mnar_mean.intercept_,ols_mnar_mean.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3** How do the estimates compare when performing mean imputation vs. just dropping rows?  Have things gotten better or worse (for what types of missingness)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression Imputation \n",
    "\n",
    "This is difficult to keep straight.  There are two models here: \n",
    "\n",
    "1. an imputation model based on OLS concerning just the predictors (to predict $X_1$ from $X_2$) and \n",
    "2. the model we really care about to predict $Y$ from the 'improved' $X_1$ (now with imputed values) and $X_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mcar = X_mcar_raw.copy()\n",
    "\n",
    "# fit the imputation model\n",
    "ols_imputer_mcar = LinearRegression().fit(X_mcar.dropna()[['x2']],X_mcar.dropna()['x1'])\n",
    "\n",
    "# perform some imputations\n",
    "yhat_impute = pd.Series(ols_imputer_mcar.predict(X_mcar[['x2']]))\n",
    "X_mcar['x1'] = X_mcar['x1'].fillna(yhat_impute)\n",
    "\n",
    "# fit the model we care about\n",
    "ols_mcar_ols = LinearRegression().fit(X_mcar,y)\n",
    "print(ols_mcar_ols.intercept_,ols_mcar_ols.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mar = X_mar_raw.copy()\n",
    "ols_imputer_mar = LinearRegression().fit(X_mar.dropna()[['x2']],X_mar.dropna()['x1'])\n",
    "\n",
    "yhat_impute = pd.Series(ols_imputer_mar.predict(X_mar[['x2']]))\n",
    "X_mar['x1'] = X_mar['x1'].fillna(yhat_impute)\n",
    "\n",
    "ols_mar_ols = LinearRegression().fit(X_mar,y)\n",
    "print(ols_mar_ols.intercept_,ols_mar_ols.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_mnar = X_mnar_raw.copy()\n",
    "ols_imputer_mnar = LinearRegression().fit(X_mnar.dropna()[['x2']],X_mnar.dropna()['x1'])\n",
    "\n",
    "yhat_impute = pd.Series(ols_imputer_mnar.predict(X_mnar[['x2']]))\n",
    "X_mnar['x1'] = X_mnar['x1'].fillna(yhat_impute)\n",
    "\n",
    "ols_mnar_ols = LinearRegression().fit(X_mnar,y)\n",
    "print(ols_mnar_ols.intercept_,ols_mnar_ols.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4**: How do the estimates compare when performing model-based imputation vs. mean imputation?  Have things gotten better or worse (for what types of missingness)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $k$-NN Imputation ($k$=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mcar = X_mcar_raw.copy()\n",
    "X_mcar = KNNImputer(n_neighbors=3).fit_transform(X_mcar)\n",
    "\n",
    "ols_mcar_knn = LinearRegression().fit(X_mcar,y)\n",
    "print(ols_mcar_knn.intercept_,ols_mcar_knn.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mar = X_mar_raw.copy()\n",
    "X_mar = KNNImputer(n_neighbors=3).fit_transform(X_mar)\n",
    "\n",
    "ols_mar_knn = LinearRegression().fit(X_mar,y)\n",
    "print(ols_mar_knn.intercept_,ols_mar_knn.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mnar = X_mnar_raw.copy()\n",
    "X_mnar = KNNImputer(n_neighbors=3).fit_transform(X_mnar)\n",
    "\n",
    "ols_mnar_knn = LinearRegression().fit(X_mnar,y)\n",
    "print(ols_mnar_knn.intercept_,ols_mnar_knn.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q5**: Which of the 4 methods for handling missingness worked best?  Which worked the worst?  Were the estimates improved or worsened in each of the 3 types of missingness?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q6**: This exercise focused on 'inference' (considering just the estimates of coefficients, not the uncertainty of these estimates, which would be even worse).  What are the ramifications on prediction?  Is the situation more or less concerning?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
