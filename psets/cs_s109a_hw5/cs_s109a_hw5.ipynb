{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://github.com/Harvard-IACS/2021-s109a/blob/master/lectures/crest.png?raw=true\"> CS-S109A Introduction to Data Science \n",
    "\n",
    "## Homework 5: Trees, Forests, and Boosting\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Summer 2021**<br/>\n",
    "**Instructors**: Kevin Rader\n",
    "\n",
    "\n",
    "<hr style='height:2px'>\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "blockquote { background: #AEDE94; }\n",
       "h1 { \n",
       "    padding-top: 25px;\n",
       "    padding-bottom: 25px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "h2 { \n",
       "    padding-top: 10px;\n",
       "    padding-bottom: 10px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "\n",
       "div.exercise {\n",
       "\tbackground-color: #ffcccc;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "\n",
       "div.exercise-r {\n",
       "\tbackground-color: #fce8e8;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "\n",
       "\n",
       "span.sub-q {\n",
       "\tfont-weight: bold;\n",
       "}\n",
       "div.theme {\n",
       "\tbackground-color: #DDDDDD;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 18pt;\n",
       "}\n",
       "div.gc { \n",
       "\tbackground-color: #AEDE94;\n",
       "\tborder-color: #E9967A; \t \n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 12pt;\n",
       "}\n",
       "p.q1 { \n",
       "    padding-top: 5px;\n",
       "    padding-bottom: 5px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "header {\n",
       "   padding-top: 35px;\n",
       "    padding-bottom: 35px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "</style>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## RUN THIS CELL TO GET THE RIGHT FORMATTING \n",
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get(\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\").text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INSTRUCTIONS\n",
    "\n",
    "- To submit your assignment follow the instructions given in Canvas.\n",
    "- Restart the kernel and run the whole notebook again before you submit. \n",
    "- If you submit individually and you have worked with someone, please include the name of your [one] partner below. \n",
    "- As much as possible, try and stick to the hints and functions we import at the top of the homework, as those are the ideas and tools the class supports and is aiming to teach. And if a problem specifies a particular library you're required to use that library, and possibly others from the import list.\n",
    "- Please use .head() when viewing data. Do not submit a notebook that is excessively long because output was not suppressed or otherwise limited. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.utils import resample\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(style='whitegrid')\n",
    "pd.set_option('display.width', 1500)\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2pt\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Higgs Boson Discovery\n",
    "\n",
    "The discovery of the Higgs boson in July 2012 marked a fundamental breakthrough in particle physics. The Higgs boson particle was discovered through experiments at the Large Hadron Collider at CERN, by colliding beams of protons at high energy. A key challenge in analyzing the results of these experiments is to differentiate between collisions that produce Higgs bosons and collisions that produce only background noise. \n",
    "\n",
    "You are provided with data from Monte-Carlo simulations of collisions of particles in a particle collider experiment. The training set is available in `Higgs_train.csv` and the test set is in `Higgs_test.csv`. Each row in these files corresponds to a particle collision described by 28 features (columns 1-28), of which the first 21 features are kinematic properties measured by the particle detectors in the accelerator, and the remaining features are derived by physicists from the first 21 features. The class label is provided in the last column, with a label of 1 indicating that the collision produces Higgs bosons (signal), and a label of 0 indicating that the collision produces only other particles (background).\n",
    "\n",
    "The data set provided to you is a small subset of the HIGGS data set in the UCI machine learning repository. The following paper contains further details about the data set and the predictors used: [Baldi et al., Nature Communications 5, 2014](https://www.nature.com/articles/ncomms5308)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 training observations, 5000 test observations\n",
      "29 total columns\n",
      "\n",
      "Columns:\n",
      "lepton pT, lepton eta, lepton phi, missing energy magnitude, missing energy phi, jet 1 pt, jet 1 eta, jet 1 phi, jet 1 b-tag, jet 2 pt, jet 2 eta, jet 2 phi, jet 2 b-tag, jet 3 pt, jet 3 eta, jet 3 phi, jet 3 b-tag, jet 4 pt, jet 4 eta, jet 4 phi, jet 4 b-tag, m_jj, m_jjj, m_lv, m_jlv, m_bb, m_wbb, m_wwbb, class\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "data_train = pd.read_csv('data/Higgs_train.csv')\n",
    "data_test = pd.read_csv('data/Higgs_test.csv')\n",
    "\n",
    "print(f\"{len(data_train)} training observations, {len(data_test)} test observations\")\n",
    "print(data_train.shape[1],\"total columns\")\n",
    "print(\"\\nColumns:\")\n",
    "print(', '.join(data_train.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lepton pT</th>\n",
       "      <th>lepton eta</th>\n",
       "      <th>lepton phi</th>\n",
       "      <th>missing energy magnitude</th>\n",
       "      <th>missing energy phi</th>\n",
       "      <th>jet 1 pt</th>\n",
       "      <th>jet 1 eta</th>\n",
       "      <th>jet 1 phi</th>\n",
       "      <th>jet 1 b-tag</th>\n",
       "      <th>jet 2 pt</th>\n",
       "      <th>jet 2 eta</th>\n",
       "      <th>jet 2 phi</th>\n",
       "      <th>jet 2 b-tag</th>\n",
       "      <th>jet 3 pt</th>\n",
       "      <th>jet 3 eta</th>\n",
       "      <th>jet 3 phi</th>\n",
       "      <th>jet 3 b-tag</th>\n",
       "      <th>jet 4 pt</th>\n",
       "      <th>jet 4 eta</th>\n",
       "      <th>jet 4 phi</th>\n",
       "      <th>jet 4 b-tag</th>\n",
       "      <th>m_jj</th>\n",
       "      <th>m_jjj</th>\n",
       "      <th>m_lv</th>\n",
       "      <th>m_jlv</th>\n",
       "      <th>m_bb</th>\n",
       "      <th>m_wbb</th>\n",
       "      <th>m_wwbb</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.884</td>\n",
       "      <td>-0.462</td>\n",
       "      <td>0.196</td>\n",
       "      <td>1.330</td>\n",
       "      <td>1.520</td>\n",
       "      <td>1.040</td>\n",
       "      <td>-1.5200</td>\n",
       "      <td>-1.46</td>\n",
       "      <td>2.17</td>\n",
       "      <td>0.361</td>\n",
       "      <td>-2.030</td>\n",
       "      <td>-0.253</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.337</td>\n",
       "      <td>-0.597</td>\n",
       "      <td>-0.324</td>\n",
       "      <td>2.55</td>\n",
       "      <td>0.493</td>\n",
       "      <td>1.030</td>\n",
       "      <td>-1.1400</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.450</td>\n",
       "      <td>0.791</td>\n",
       "      <td>1.400</td>\n",
       "      <td>1.250</td>\n",
       "      <td>0.713</td>\n",
       "      <td>0.812</td>\n",
       "      <td>0.821</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.780</td>\n",
       "      <td>-0.292</td>\n",
       "      <td>0.897</td>\n",
       "      <td>0.384</td>\n",
       "      <td>0.413</td>\n",
       "      <td>1.220</td>\n",
       "      <td>-0.4660</td>\n",
       "      <td>-0.92</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.100</td>\n",
       "      <td>-1.200</td>\n",
       "      <td>1.080</td>\n",
       "      <td>2.21</td>\n",
       "      <td>0.498</td>\n",
       "      <td>1.490</td>\n",
       "      <td>1.060</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.493</td>\n",
       "      <td>-0.344</td>\n",
       "      <td>-0.0918</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.812</td>\n",
       "      <td>0.728</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.637</td>\n",
       "      <td>0.569</td>\n",
       "      <td>0.777</td>\n",
       "      <td>0.715</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.353</td>\n",
       "      <td>-1.070</td>\n",
       "      <td>-1.740</td>\n",
       "      <td>1.170</td>\n",
       "      <td>-0.199</td>\n",
       "      <td>0.558</td>\n",
       "      <td>0.0574</td>\n",
       "      <td>-1.49</td>\n",
       "      <td>1.09</td>\n",
       "      <td>0.912</td>\n",
       "      <td>-1.080</td>\n",
       "      <td>0.571</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.781</td>\n",
       "      <td>-0.320</td>\n",
       "      <td>-1.040</td>\n",
       "      <td>2.55</td>\n",
       "      <td>0.616</td>\n",
       "      <td>-1.510</td>\n",
       "      <td>1.5500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.829</td>\n",
       "      <td>1.060</td>\n",
       "      <td>0.992</td>\n",
       "      <td>0.825</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.766</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.757</td>\n",
       "      <td>0.822</td>\n",
       "      <td>-1.290</td>\n",
       "      <td>0.208</td>\n",
       "      <td>-0.151</td>\n",
       "      <td>1.220</td>\n",
       "      <td>-1.6400</td>\n",
       "      <td>1.53</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.680</td>\n",
       "      <td>0.189</td>\n",
       "      <td>-0.645</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.780</td>\n",
       "      <td>0.285</td>\n",
       "      <td>0.520</td>\n",
       "      <td>2.55</td>\n",
       "      <td>1.130</td>\n",
       "      <td>-0.932</td>\n",
       "      <td>0.1170</td>\n",
       "      <td>3.1</td>\n",
       "      <td>4.290</td>\n",
       "      <td>2.420</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.923</td>\n",
       "      <td>0.927</td>\n",
       "      <td>1.760</td>\n",
       "      <td>1.360</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.040</td>\n",
       "      <td>2.030</td>\n",
       "      <td>-0.472</td>\n",
       "      <td>0.424</td>\n",
       "      <td>-1.500</td>\n",
       "      <td>1.060</td>\n",
       "      <td>0.7980</td>\n",
       "      <td>1.22</td>\n",
       "      <td>2.17</td>\n",
       "      <td>0.805</td>\n",
       "      <td>-0.661</td>\n",
       "      <td>-1.460</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.885</td>\n",
       "      <td>1.440</td>\n",
       "      <td>0.809</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.290</td>\n",
       "      <td>0.493</td>\n",
       "      <td>-0.0791</td>\n",
       "      <td>3.1</td>\n",
       "      <td>0.895</td>\n",
       "      <td>0.936</td>\n",
       "      <td>1.030</td>\n",
       "      <td>1.560</td>\n",
       "      <td>1.150</td>\n",
       "      <td>1.120</td>\n",
       "      <td>1.160</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   lepton pT  lepton eta  lepton phi  missing energy magnitude  missing energy phi  jet 1 pt  jet 1 eta  jet 1 phi  jet 1 b-tag  jet 2 pt  jet 2 eta  jet 2 phi  jet 2 b-tag  jet 3 pt  jet 3 eta  jet 3 phi  jet 3 b-tag  jet 4 pt  jet 4 eta  jet 4 phi  jet 4 b-tag   m_jj  m_jjj   m_lv  m_jlv   m_bb  m_wbb  m_wwbb  class\n",
       "0      0.884      -0.462       0.196                     1.330               1.520     1.040    -1.5200      -1.46         2.17     0.361     -2.030     -0.253         0.00     0.337     -0.597     -0.324         2.55     0.493      1.030    -1.1400          0.0  1.450  0.791  1.400  1.250  0.713  0.812   0.821    1.0\n",
       "1      0.780      -0.292       0.897                     0.384               0.413     1.220    -0.4660      -0.92         0.00     1.100     -1.200      1.080         2.21     0.498      1.490      1.060         0.00     0.493     -0.344    -0.0918          0.0  0.812  0.728  0.975  0.637  0.569  0.777   0.715    1.0\n",
       "2      0.353      -1.070      -1.740                     1.170              -0.199     0.558     0.0574      -1.49         1.09     0.912     -1.080      0.571         0.00     0.781     -0.320     -1.040         2.55     0.616     -1.510     1.5500          0.0  0.829  1.060  0.992  0.825  0.365  0.800   0.766    0.0\n",
       "3      0.757       0.822      -1.290                     0.208              -0.151     1.220    -1.6400       1.53         0.00     1.680      0.189     -0.645         0.00     1.780      0.285      0.520         2.55     1.130     -0.932     0.1170          3.1  4.290  2.420  0.995  0.923  0.927  1.760   1.360    1.0\n",
       "4      2.040       2.030      -0.472                     0.424              -1.500     1.060     0.7980       1.22         2.17     0.805     -0.661     -1.460         0.00     0.885      1.440      0.809         0.00     1.290      0.493    -0.0791          3.1  0.895  0.936  1.030  1.560  1.150  1.120   1.160    1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lepton pT</th>\n",
       "      <th>lepton eta</th>\n",
       "      <th>lepton phi</th>\n",
       "      <th>missing energy magnitude</th>\n",
       "      <th>missing energy phi</th>\n",
       "      <th>jet 1 pt</th>\n",
       "      <th>jet 1 eta</th>\n",
       "      <th>jet 1 phi</th>\n",
       "      <th>jet 1 b-tag</th>\n",
       "      <th>jet 2 pt</th>\n",
       "      <th>jet 2 eta</th>\n",
       "      <th>jet 2 phi</th>\n",
       "      <th>jet 2 b-tag</th>\n",
       "      <th>jet 3 pt</th>\n",
       "      <th>jet 3 eta</th>\n",
       "      <th>jet 3 phi</th>\n",
       "      <th>jet 3 b-tag</th>\n",
       "      <th>jet 4 pt</th>\n",
       "      <th>jet 4 eta</th>\n",
       "      <th>jet 4 phi</th>\n",
       "      <th>jet 4 b-tag</th>\n",
       "      <th>m_jj</th>\n",
       "      <th>m_jjj</th>\n",
       "      <th>m_lv</th>\n",
       "      <th>m_jlv</th>\n",
       "      <th>m_bb</th>\n",
       "      <th>m_wbb</th>\n",
       "      <th>m_wwbb</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.986168</td>\n",
       "      <td>0.011449</td>\n",
       "      <td>-0.011057</td>\n",
       "      <td>0.995656</td>\n",
       "      <td>-0.026783</td>\n",
       "      <td>0.990357</td>\n",
       "      <td>0.006779</td>\n",
       "      <td>0.008310</td>\n",
       "      <td>0.999370</td>\n",
       "      <td>0.992922</td>\n",
       "      <td>0.015321</td>\n",
       "      <td>0.010629</td>\n",
       "      <td>1.014648</td>\n",
       "      <td>0.984070</td>\n",
       "      <td>0.017681</td>\n",
       "      <td>-0.012232</td>\n",
       "      <td>0.992122</td>\n",
       "      <td>0.982671</td>\n",
       "      <td>-0.020045</td>\n",
       "      <td>0.017153</td>\n",
       "      <td>0.996650</td>\n",
       "      <td>1.024146</td>\n",
       "      <td>1.023271</td>\n",
       "      <td>1.047602</td>\n",
       "      <td>1.005986</td>\n",
       "      <td>0.972397</td>\n",
       "      <td>1.027461</td>\n",
       "      <td>0.954268</td>\n",
       "      <td>0.529600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.550671</td>\n",
       "      <td>1.015018</td>\n",
       "      <td>1.012867</td>\n",
       "      <td>0.601449</td>\n",
       "      <td>1.012443</td>\n",
       "      <td>0.483224</td>\n",
       "      <td>1.005735</td>\n",
       "      <td>0.992744</td>\n",
       "      <td>1.023674</td>\n",
       "      <td>0.499675</td>\n",
       "      <td>0.994269</td>\n",
       "      <td>1.007182</td>\n",
       "      <td>1.046952</td>\n",
       "      <td>0.478845</td>\n",
       "      <td>1.012092</td>\n",
       "      <td>1.007317</td>\n",
       "      <td>1.198333</td>\n",
       "      <td>0.505908</td>\n",
       "      <td>1.005818</td>\n",
       "      <td>1.020375</td>\n",
       "      <td>1.395097</td>\n",
       "      <td>0.596476</td>\n",
       "      <td>0.352384</td>\n",
       "      <td>0.155067</td>\n",
       "      <td>0.402897</td>\n",
       "      <td>0.515233</td>\n",
       "      <td>0.360030</td>\n",
       "      <td>0.314996</td>\n",
       "      <td>0.499173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.275000</td>\n",
       "      <td>-2.390000</td>\n",
       "      <td>-1.740000</td>\n",
       "      <td>0.014000</td>\n",
       "      <td>-1.740000</td>\n",
       "      <td>0.236000</td>\n",
       "      <td>-2.960000</td>\n",
       "      <td>-1.740000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.190000</td>\n",
       "      <td>-2.870000</td>\n",
       "      <td>-1.740000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.265000</td>\n",
       "      <td>-2.730000</td>\n",
       "      <td>-1.740000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.366000</td>\n",
       "      <td>-2.490000</td>\n",
       "      <td>-1.740000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.161000</td>\n",
       "      <td>0.310000</td>\n",
       "      <td>0.493000</td>\n",
       "      <td>0.445000</td>\n",
       "      <td>0.048100</td>\n",
       "      <td>0.417000</td>\n",
       "      <td>0.448000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.584000</td>\n",
       "      <td>-0.729000</td>\n",
       "      <td>-0.891250</td>\n",
       "      <td>0.569000</td>\n",
       "      <td>-0.901000</td>\n",
       "      <td>0.676000</td>\n",
       "      <td>-0.688250</td>\n",
       "      <td>-0.840000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.661000</td>\n",
       "      <td>-0.688250</td>\n",
       "      <td>-0.859250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.651000</td>\n",
       "      <td>-0.686250</td>\n",
       "      <td>-0.873250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.616000</td>\n",
       "      <td>-0.728000</td>\n",
       "      <td>-0.884500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.791750</td>\n",
       "      <td>0.849000</td>\n",
       "      <td>0.986000</td>\n",
       "      <td>0.766750</td>\n",
       "      <td>0.675000</td>\n",
       "      <td>0.819000</td>\n",
       "      <td>0.767000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.855000</td>\n",
       "      <td>0.013600</td>\n",
       "      <td>-0.023250</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>-0.031950</td>\n",
       "      <td>0.888000</td>\n",
       "      <td>0.012800</td>\n",
       "      <td>0.030100</td>\n",
       "      <td>1.090000</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>0.014150</td>\n",
       "      <td>0.019600</td>\n",
       "      <td>1.110000</td>\n",
       "      <td>0.887000</td>\n",
       "      <td>0.003810</td>\n",
       "      <td>-0.007130</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.871000</td>\n",
       "      <td>-0.033800</td>\n",
       "      <td>0.024800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.892000</td>\n",
       "      <td>0.957000</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.941000</td>\n",
       "      <td>0.865000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.230000</td>\n",
       "      <td>0.771000</td>\n",
       "      <td>0.860250</td>\n",
       "      <td>1.290000</td>\n",
       "      <td>0.854000</td>\n",
       "      <td>1.170000</td>\n",
       "      <td>0.702000</td>\n",
       "      <td>0.850250</td>\n",
       "      <td>2.170000</td>\n",
       "      <td>1.210000</td>\n",
       "      <td>0.691250</td>\n",
       "      <td>0.871250</td>\n",
       "      <td>2.210000</td>\n",
       "      <td>1.210000</td>\n",
       "      <td>0.717000</td>\n",
       "      <td>0.863000</td>\n",
       "      <td>2.550000</td>\n",
       "      <td>1.220000</td>\n",
       "      <td>0.714250</td>\n",
       "      <td>0.917250</td>\n",
       "      <td>3.100000</td>\n",
       "      <td>1.030000</td>\n",
       "      <td>1.090000</td>\n",
       "      <td>1.020000</td>\n",
       "      <td>1.130000</td>\n",
       "      <td>1.140000</td>\n",
       "      <td>1.130000</td>\n",
       "      <td>1.050000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.960000</td>\n",
       "      <td>2.400000</td>\n",
       "      <td>1.740000</td>\n",
       "      <td>5.510000</td>\n",
       "      <td>1.740000</td>\n",
       "      <td>5.320000</td>\n",
       "      <td>2.960000</td>\n",
       "      <td>1.740000</td>\n",
       "      <td>2.170000</td>\n",
       "      <td>4.930000</td>\n",
       "      <td>2.900000</td>\n",
       "      <td>1.740000</td>\n",
       "      <td>2.210000</td>\n",
       "      <td>5.410000</td>\n",
       "      <td>2.710000</td>\n",
       "      <td>1.740000</td>\n",
       "      <td>2.550000</td>\n",
       "      <td>7.510000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>1.740000</td>\n",
       "      <td>3.100000</td>\n",
       "      <td>9.280000</td>\n",
       "      <td>5.180000</td>\n",
       "      <td>3.930000</td>\n",
       "      <td>4.800000</td>\n",
       "      <td>5.560000</td>\n",
       "      <td>4.690000</td>\n",
       "      <td>5.990000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         lepton pT   lepton eta   lepton phi  missing energy magnitude  missing energy phi     jet 1 pt    jet 1 eta    jet 1 phi  jet 1 b-tag     jet 2 pt    jet 2 eta    jet 2 phi  jet 2 b-tag     jet 3 pt    jet 3 eta    jet 3 phi  jet 3 b-tag     jet 4 pt    jet 4 eta    jet 4 phi  jet 4 b-tag         m_jj        m_jjj         m_lv        m_jlv         m_bb        m_wbb       m_wwbb        class\n",
       "count  5000.000000  5000.000000  5000.000000               5000.000000         5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000\n",
       "mean      0.986168     0.011449    -0.011057                  0.995656           -0.026783     0.990357     0.006779     0.008310     0.999370     0.992922     0.015321     0.010629     1.014648     0.984070     0.017681    -0.012232     0.992122     0.982671    -0.020045     0.017153     0.996650     1.024146     1.023271     1.047602     1.005986     0.972397     1.027461     0.954268     0.529600\n",
       "std       0.550671     1.015018     1.012867                  0.601449            1.012443     0.483224     1.005735     0.992744     1.023674     0.499675     0.994269     1.007182     1.046952     0.478845     1.012092     1.007317     1.198333     0.505908     1.005818     1.020375     1.395097     0.596476     0.352384     0.155067     0.402897     0.515233     0.360030     0.314996     0.499173\n",
       "min       0.275000    -2.390000    -1.740000                  0.014000           -1.740000     0.236000    -2.960000    -1.740000     0.000000     0.190000    -2.870000    -1.740000     0.000000     0.265000    -2.730000    -1.740000     0.000000     0.366000    -2.490000    -1.740000     0.000000     0.161000     0.310000     0.493000     0.445000     0.048100     0.417000     0.448000     0.000000\n",
       "25%       0.584000    -0.729000    -0.891250                  0.569000           -0.901000     0.676000    -0.688250    -0.840000     0.000000     0.661000    -0.688250    -0.859250     0.000000     0.651000    -0.686250    -0.873250     0.000000     0.616000    -0.728000    -0.884500     0.000000     0.791750     0.849000     0.986000     0.766750     0.675000     0.819000     0.767000     0.000000\n",
       "50%       0.855000     0.013600    -0.023250                  0.890000           -0.031950     0.888000     0.012800     0.030100     1.090000     0.890000     0.014150     0.019600     1.110000     0.887000     0.003810    -0.007130     0.000000     0.871000    -0.033800     0.024800     0.000000     0.892000     0.957000     0.990000     0.910000     0.875000     0.941000     0.865000     1.000000\n",
       "75%       1.230000     0.771000     0.860250                  1.290000            0.854000     1.170000     0.702000     0.850250     2.170000     1.210000     0.691250     0.871250     2.210000     1.210000     0.717000     0.863000     2.550000     1.220000     0.714250     0.917250     3.100000     1.030000     1.090000     1.020000     1.130000     1.140000     1.130000     1.050000     1.000000\n",
       "max       4.960000     2.400000     1.740000                  5.510000            1.740000     5.320000     2.960000     1.740000     2.170000     4.930000     2.900000     1.740000     2.210000     5.410000     2.710000     1.740000     2.550000     7.510000     2.500000     1.740000     3.100000     9.280000     5.180000     3.930000     4.800000     5.560000     4.690000     5.990000     1.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(data_train.head())\n",
    "display(data_train.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into NumPy arrays\n",
    "X_train = data_train.iloc[:, data_train.columns != 'class'].values\n",
    "y_train = data_train['class'].values\n",
    "X_test = data_test.iloc[:, data_test.columns != 'class'].values\n",
    "y_test = data_test['class'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'><b>Question 1 [25 pts]: Base Modeling </b></div>\n",
    "We begin by looking at parametric approaches to modeling the Boson data.\n",
    "\n",
    "**1.1** Calculate the correlation matrix (using `df.corr()` from pandas) between all predictors in the training data set and represent this as a heatmap using (`seaborn.heatmap`).  What does the suggest about the presence of muilticollinearity in the predictor set?\n",
    "\n",
    "**1.2** Fit a logistic regression model to predict `class` from all 28 of the predictors, and evaluate the model's accuracy on both the training and test sets.\n",
    "\n",
    "**1.3** Fit a logistic regression model to predict `class` from all 28 of the predictors and all two-way interactions between these predictors (you should have $28+{28 \\choose 2} = 406$ columns in the design matrix, ignoring the intercept).  Evaluate the model's accuracy on both the training and test sets.\n",
    "\n",
    "Hint: you may have to scale your predictors (purely for numerical reasons) and use`maxiter=2000` to avoid the warnings (the next part as well).\n",
    "\n",
    "**1.4** Use 'L2' regularization on the model in the previous part, tuning the penalty term using 5-fold cross-validation (consider the set of Cs to be [0.01,0.1,1,10,100,1000]).  Which penalty term performed best?  Evaluate the best model's accuracy on both the training and test sets and assign them to variables named `logistic_train_score` and `logistic_test_score`.\n",
    "\n",
    "**1.5** Compare the performance of the 3 models above.  Which model performs the best on the train set?  Which the best on the test set?  Is this expected?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1** Calculate the correlation matrix (using `df.corr()` from pandas) between all predictors in the training data set and represent this as a heatmap using (`seaborn.heatmap`).  What does the suggest about the presence of muilticollinearity in the predictor set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "columns = list(data_train.columns)\n",
    "columns.remove('class')\n",
    "heatmap = sns.heatmap(pd.DataFrame(X_train, columns=columns).corr());\n",
    "heatmap.set_title('Correlation Heatmap', fontdict={'fontsize':12}, pad=12);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here a healthy mix of positive and negative correlation between variables which suggests that there exists a slight degree of multicollinearity in the predictor sets. More notably, `m_wbb` and\t`m_wwbb` seem to have a higher degree of correlation than the others.\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2** Fit a logistic regression model to predict `class` from all 28 of the predictors, and evaluate the model's accuracy on both the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Classification Accuracy: 0.6546\n",
      "Test Classification Accuracy: 0.6288\n"
     ]
    }
   ],
   "source": [
    "logistic_model = LogisticRegression(fit_intercept=True, penalty=\"none\").fit(X_train, y_train)\n",
    "\n",
    "train_accuracy = logistic_model.score(X_train, y_train)\n",
    "test_accuracy = logistic_model.score(X_test, y_test)\n",
    "\n",
    "print(\"Training Classification Accuracy: {}\".format(train_accuracy))\n",
    "print(\"Test Classification Accuracy: {}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3** Fit a logistic regression model to predict `class` from all 28 of the predictors and all two-way interactions between these predictors (you should have $28+{28 \\choose 2} = 406$ columns in the design matrix, ignoring the intercept).  Evaluate the model's accuracy on both the training and test sets.\n",
    "\n",
    "Hint: you may have to scale your predictors (purely for numerical reasons) and use`maxiter=2000` to avoid the warnings (the next part as well)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Classification Accuracy: 0.7094\n",
      "Test Classification Accuracy: 0.6312\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# scale the data\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# add interaction terms\n",
    "interaction_terms = PolynomialFeatures(interaction_only=True, include_bias = False)\n",
    "X_train_scaled_with_interactions = interaction_terms.fit_transform(X_train_scaled)\n",
    "X_test_scaled_with_interactions = interaction_terms.fit_transform(X_test_scaled)\n",
    "\n",
    "logistic_interaction_model = LogisticRegression(max_iter=2000,\n",
    "                                                fit_intercept=True, \n",
    "                                                penalty=\"none\").fit(X_train_scaled_with_interactions, y_train)\n",
    "\n",
    "# get the accuracy\n",
    "train_scaled_accuracy = logistic_interaction_model.score(X_train_scaled_with_interactions, y_train)\n",
    "test_scaled_accuracy = logistic_interaction_model.score(X_test_scaled_with_interactions, y_test)\n",
    "\n",
    "print(\"Training Classification Accuracy: {}\".format(train_scaled_accuracy))\n",
    "print(\"Test Classification Accuracy: {}\".format(test_scaled_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.4** Use 'L2' regularization on the model in the previous part, tuning the penalty term using 5-fold cross-validation (consider the set of Cs to be [0.01,0.1,1,10,100,1000]).  Which penalty term performed best?  Evaluate the best model's accuracy on both the training and test sets. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Classification Accuracy: 0.7098\n",
      "Test Classification Accuracy: 0.6302\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "# reg_log_interact_model = LogisticRegressionCV(Cs=[1],\n",
    "#                                               max_iter=2000,\n",
    "#                                               fit_intercept=True,\n",
    "#                                               cv=5,\n",
    "#                                               penalty=\"l2\").fit(X_train_scaled_with_interactions, y_train)\n",
    "\n",
    "# # get the accuracy\n",
    "# train_reg_log_accuracy = reg_log_interact_model.score(X_train_scaled_with_interactions, y_train)\n",
    "# test_reg_log_accuracy = reg_log_interact_model.score(X_test_scaled_with_interactions, y_test)\n",
    "\n",
    "# print(\"Training Classification Accuracy: {}\".format(train_reg_log_accuracy))\n",
    "# print(\"Test Classification Accuracy: {}\".format(test_reg_log_accuracy))\n",
    "#0.6302"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimum c will be: 0.01\n",
      "Training Classification Accuracy: 0.6978\n",
      "Test Classification Accuracy: 0.627\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "max_acc = -float('inf')\n",
    "optimum_c = float('inf')\n",
    "\n",
    "for c in [0.01,0.1,1,10,100,1000]:\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    scores = []\n",
    "    for train_index, valid_index in kf.split(X_train_scaled_with_interactions):\n",
    "        X_tr, X_validation = X_train_scaled_with_interactions[train_index], X_train_scaled_with_interactions[valid_index]\n",
    "        y_tr, y_validation = y_train[train_index], y_test[valid_index]\n",
    "        model = LogisticRegression(max_iter=2000, penalty='l2', C=c).fit(X_tr, y_tr)\n",
    "        scores.append(model.score(X_validation, y_validation))\n",
    "        \n",
    "    model_score = np.mean(scores)\n",
    "    \n",
    "    if max_acc < model_score:\n",
    "        optimum_c = c\n",
    "        max_acc = model_score\n",
    "        \n",
    "print(\"The optimum c will be:\", optimum_c)\n",
    "\n",
    "reg_log_interact_model = LogisticRegression(max_iter=2000, penalty='l2', C=optimum_c).fit(X_train_scaled_with_interactions, y_train)\n",
    "\n",
    "# get the accuracy\n",
    "train_reg_log_accuracy = reg_log_interact_model.score(X_train_scaled_with_interactions, y_train)\n",
    "test_reg_log_accuracy = reg_log_interact_model.score(X_test_scaled_with_interactions, y_test)\n",
    "\n",
    "print(\"Training Classification Accuracy: {}\".format(train_reg_log_accuracy))\n",
    "print(\"Test Classification Accuracy: {}\".format(test_reg_log_accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.5** Compare the performance of the 3 models above.  Which model performs the best on the train set?  Which the best on the test set?  Is this expected?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'><b>Question 2 [30 pts]: Tree and Forest Models </b></div>\n",
    "Our first go to tree-based models, along with our first ensemble methods (bagging and random forests) which we played around with a bit on homework 4.\n",
    "\n",
    "**2.1** Fit a decision tree model to the training set. Choose a range of tree depths from 1 to 20 and evaluate the  performance and standard deviations for each depth using 5-fold cross-validation. Plot the estimated mean +/- 2 standard deviations for each depth. Also, include the training set performance in your plot, but set the y-axis to focus on the cross-validation performance. \n",
    "Store the CV means and std variables `cvmeans`, `cvstds` and the train score `train_scores`  \n",
    "\n",
    "*Hint*: use `plt.fill_between` to shade the region.\n",
    "\n",
    "**2.2** Select an appropriate depth and justify your choice using your cross-validation estimates. Then report the classification accuracy on the **test set**. Store the training and test accuracies in variables named `best_cv_tree_train_score` and `best_cv_tree_test_score` to refer to in a later question.\n",
    "\n",
    "**2.3** Fit a Bagging model using the `X_train` data.  Tune (1) the tree depth from the depths of [5.10.15], and (2) the number of trees from the options [20,50,100].  Evaluate its accuracy on the train and test sets and assign it to variables named `bagging_train_score` and `bagging_test_score`.\n",
    "\n",
    "**2.4** Fit a `RandomForestClassifier` using the original `X_train` data using the same tree depth and number of trees that you used in the previous question, but tune the number features to consider when looking for the best splits (consider from the list [5,10,15,20,25]). Evaluate its accuracy on the train and test sets and assign it to variables named `random_forest_train_score` and `random_forest_test_score`.\n",
    "\n",
    "**2.5** Why should the bagging model be tuned to a `max_depth` that is at least as deep as the best single tree depth (from 2.2)?\n",
    "\n",
    "**2.6**: If you were to run your code again for the bagging and random forest models, would you get the same results?  Why?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1** Fit a decision tree model to the training set. Choose a range of tree depths from 1 to 20 and evaluate the  performance and standard deviations for each depth using 5-fold cross-validation. Plot the estimated mean +/- 2 standard deviations for each depth. Also, include the training set performance in your plot, but set the y-axis to focus on the cross-validation performance. \n",
    "Store the CV means and std variables `cvmeans`, `cvstds` and the train score `train_scores`  \n",
    "\n",
    "*Hint*: use `plt.fill_between` to shade the region.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "# your code here\n",
    "######\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2** Select an appropriate depth and justify your choice using your cross-validation estimates. Then report the classification accuracy on the **test set**. Store the training and test accuracies in variables named `best_cv_tree_train_score` and `best_cv_tree_test_score` to refer to in a later question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "# your code here\n",
    "######\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3** Fit a Bagging model using the `X_train` data.  Tune (1) the tree depth from the depths of [5.10.15], and (2) the number of trees from the options [20,50,100].  Evaluate its accuracy on the train and test sets and assign it to variables named `bagging_train_score` and `bagging_test_score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "# your code here\n",
    "######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.4** Fit a `RandomForestClassifier` using the original `X_train` data using the same tree depth and number of trees that you used in the previous question, but tune the number features to consider when looking for the best splits (consider from the list [5,10,15,20,25]). Evaluate its accuracy on the train and test sets and assign it to variables named `random_forest_train_score` and `random_forest_test_score`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "# your code here\n",
    "######\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.5** Why should the bagging model be tuned to a `max_depth` that is at least as deep as the best single tree depth (from 2.2)?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.6**: If you were to run your code again for the bagging and random forest models, would you get the same results?  Why?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'><b> Question 3 [30 pts]: Boosting </div>\n",
    "In this question we explore a different kind of ensemble method, boosting, where each new model is trained on a dataset weighted towards observations that the current set of models predicts incorrectly. \n",
    "\n",
    "We'll focus on the AdaBoost flavor of boosting and examine what happens to the ensemble model's accuracy as the algorithm adds more estimators (iterations) to the ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1** We'll motivate AdaBoost by noticing patterns in the errors that a single classifier makes. Fit `tree1`, a decision tree with depth 3, to the training data. \n",
    "Report the train and test accuracies. For each predictor, make a plot that compares two distributions: the values of that predictor for examples that `tree1` classifies correctly, and the values of that predictor for examples that `tree1` classifies incorrectly. Do you notice any predictors for which the distributions are clearly different?\n",
    "\n",
    "*Hints*:\n",
    "- If you have `fig, axs = plt.subplots(...)`, then `axs.ravel()` gives a list of each plot in reading order.\n",
    "- [`sns.kdeplot`](https://seaborn.pydata.org/generated/seaborn.kdeplot.html) takes `ax` and `label` parameters.\n",
    "\n",
    "**3.2** Use the sklearn implementation of AdaBoost: Use `AdaBoostClassifier` to fit another ensemble to `X_train`. Use a decision tree with `max_depth=3` as the base learner and a learning rate 0.05, and run the boosting for 800 iterations. Make a plot of the effect of the number of estimators/iterations on the model's train and test accuracy.\n",
    "\n",
    "*Hint*: The `staged_score` method provides the accuracy numbers you'll need. You'll need to use `list()` to convert the \"generator\" it returns into an ordinary list.\n",
    "\n",
    "**3.3** Repeat the plot above for a base learner with depth of (1, 2, 3, 4). What trends do you see in the training and test accuracy?\n",
    "\n",
    "(It's okay if your code re-fits the depth-3 classifier instead of reusing the results from the previous problem.)\n",
    "\n",
    "**3.4** Based on the plot you just made, what combination of base learner depth and number of iterations seems optimal? Why? How does the performance of this model compare with the performance of the ensembles you considered above?\n",
    "\n",
    "**3.5** What is the impact of having too many trees in boosting and in bagging? In which instance is it worse to have too many trees?\n",
    "\n",
    "**3.6** Which technique, boosting or bagging, is better suited to parallelization, where you could have multiple computers working on a problem at the same time?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1** We'll motivate AdaBoost by noticing patterns in the errors that a single classifier makes. Fit `tree1`, a decision tree with depth 3, to the training data. \n",
    "Report the train and test accuracies. For each predictor, make a plot that compares two distributions: the values of that predictor for examples that `tree1` classifies correctly, and the values of that predictor for examples that `tree1` classifies incorrectly. Do you notice any predictors for which the distributions are clearly different?\n",
    "\n",
    "*Hints*:\n",
    "- If you have `fig, axs = plt.subplots(...)`, then `axs.ravel()` gives a list of each plot in reading order.\n",
    "- [`sns.kdeplot`](https://seaborn.pydata.org/generated/seaborn.kdeplot.html) takes `ax` and `label` parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "# your code here\n",
    "######\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2** Use the sklearn implementation of AdaBoost: Use `AdaBoostClassifier` to fit another ensemble to `X_train`. Use a decision tree with `max_depth=3` as the base learner and a learning rate 0.05, and run the boosting for 800 iterations. Make a plot of the effect of the number of estimators/iterations on the model's train and test accuracy.\n",
    "\n",
    "*Hint*: The `staged_score` method provides the accuracy numbers you'll need. You'll need to use `list()` to convert the \"generator\" it returns into an ordinary list.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "# your code here\n",
    "######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.3** Repeat the plot above for a base learner with depth of (1, 2, 3, 4). What trends do you see in the training and test accuracy?\n",
    "\n",
    "(It's okay if your code re-fits the depth-3 classifier instead of reusing the results from the previous problem.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "# your code here\n",
    "######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.4** Based on the plot you just made, what combination of base learner depth and number of iterations seems optimal? Why? How does the performance of this model compare with the performance of the ensembles you considered above?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "# your code here\n",
    "######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.5** What is the impact of having too many trees in boosting and in bagging? In which instance is it worse to have too many trees?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.6** Which technique, boosting or bagging, is better suited to parallelization, where you could have multiple computers working on a problem at the same time?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'> <b> Question 4 [15 pts]: Model Comparison </b> </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.1**: Make a pandas table of the training and test accuracy for the following 5 models and name it `results_df`:\n",
    "\n",
    "- The tuned L2-regularized logistic regression (from Question 1)\n",
    "- Single tree with best depth chosen by cross-validation (from Question 2)\n",
    "- Your tuned bagging model (from Question 2)\n",
    "- Your tuned random forest model  (from Question 2)\n",
    "- Your tuned adaBoost model  (from Question 3)\n",
    "\n",
    "(see below for the expected structure)  \n",
    "\n",
    "(This problem should not require fitting any new models, though you may need to go back and store the accuracies from models you fit previously.)\n",
    "\n",
    "What is the relative performance of each model on the training set? On the test set? Comment on how these relationships make sense (or don't make sense) in light of how each model treats the bias-variance trade-off.\n",
    "\n",
    "\n",
    "**4.2** Reflect on the overall performance of all of the different classifiers you have seen throughout this assignment. Which performed best? Why do you think that may have happened?\n",
    "\n",
    "\n",
    "**4.3** Which of these techniques can be extended to regression tasks? How?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.1**: Make a pandas table of the training and test accuracy for the following 6 models and name it `results_df`:\n",
    "\n",
    "- The full logistic regression (from Question 1.3)\n",
    "- The tuned L2-regularized logistic regression (from Question 1.4)\n",
    "- Single tree with best depth chosen by cross-validation (from Question 2)\n",
    "- Your tuned bagging model (from Question 2)\n",
    "- Your tuned random forest model  (from Question 2)\n",
    "- Your tuned adaBoost model  (from Question 3)\n",
    "\n",
    "(see below for the expected structure)  \n",
    "\n",
    "(This problem should not require fitting any new models, though you may need to go back and store the accuracies from models you fit previously.)\n",
    "\n",
    "What is the relative performance of each model on the training set? On the test set? Comment on how these relationships make sense (or don't make sense) in light of how each model treats the bias-variance trade-off.\n",
    "\n",
    "---\n",
    "\n",
    "Fill in the following table.\n",
    "\n",
    "\n",
    "| classifier | training accuracy | test accuracy |\n",
    "| --- | --- | --- |\n",
    "| logistic  | | |\n",
    "| regularized logistic  | | |\n",
    "| single tree  | | |\n",
    "| bagging | | |\n",
    "| random forest  | | |\n",
    "| adaboost  | | |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "# your code here\n",
    "######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.2** Reflect on the overall performance of all of the different classifiers you have seen throughout this assignment. Which performed best? Why do you think that may have happened?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**4.3** Which of these techniques can be extended to regression tasks? How?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
